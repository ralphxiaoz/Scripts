#!/usr/bin/env python3
"""
This script converts exported HTML files from Confluence to Markdown format
Recursive HTML to Markdown Converter
Starts from index.html and recursively converts all linked HTML pages.
"""

import os
import sys
from pathlib import Path
import argparse
import html2text
import re
from urllib.parse import urljoin, urlparse
from collections import deque, OrderedDict
import json
from datetime import datetime

# ============================================================================
# CUSTOMIZATION SECTION - Easy to modify for specific needs
# ============================================================================

def apply_custom_html_preprocessing(html_content):
    """
    Apply custom preprocessing to HTML content before conversion.
    Modify this function to add your own HTML cleanup logic.
    
    Args:
        html_content (str): Raw HTML content
    
    Returns:
        str: Processed HTML content
    """
    try:
        # 1. Remove Confluence footer text (2 lines at end of HTML files)
        # The footer structure in your HTML:
        # <div id="footer" role="contentinfo">
        #     <section class="footer-body">
        #         <p>Document generated by Confluence on Sep 26, 2025 11:23</p>
        #         <div id="footer-logo"><a href="http://www.atlassian.com/">Atlassian</a></div>
        #     </section>
        # </div>
        
        # Pattern 1: Remove the entire footer div
        footer_div_pattern = r'<div id="footer"[^>]*>.*?</div>\s*</body>'
        html_content = re.sub(footer_div_pattern, '</body>', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # Pattern 2: Remove individual Confluence text elements (backup pattern)
        confluence_text_pattern = r'<p[^>]*>Document generated by Confluence on[^<]*</p>'
        html_content = re.sub(confluence_text_pattern, '', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # Pattern 3: Remove Atlassian link elements
        atlassian_link_pattern = r'<div[^>]*footer-logo[^>]*>.*?<a[^>]*atlassian\.com[^>]*>.*?</a>.*?</div>'
        html_content = re.sub(atlassian_link_pattern, '', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # 2. Remove breadcrumb navigation links at the top of pages
        # The breadcrumb structure:
        # <div id="breadcrumb-section">
        #     <ol id="breadcrumbs">
        #         <li class="first"><span><a href="index.html">Username</a></span></li>
        #         <li><span><a href="Page_123.html">Page Name</a></span></li>
        #     </ol>
        # </div>
        
        # Remove the entire breadcrumb section
        breadcrumb_section_pattern = r'<div[^>]*id="breadcrumb-section"[^>]*>.*?</div>'
        html_content = re.sub(breadcrumb_section_pattern, '', html_content, flags=re.IGNORECASE | re.DOTALL)
        
        # Additional HTML preprocessing can be added here
        # Example: html_content = html_content.replace('unwanted_html', '')
        
        return html_content
        
    except Exception as e:
        print(f"âš ï¸ Warning: Error in HTML preprocessing: {str(e)}")
        return html_content

def apply_custom_markdown_postprocessing(markdown_content):
    """
    Apply custom postprocessing to Markdown content after conversion.
    Modify this function to add your own Markdown cleanup logic.
    
    Args:
        markdown_content (str): Raw Markdown content from html2text
    
    Returns:
        str: Processed Markdown content
    """
    try:
        # 2. Remove user-specific prefixes (can be customized)
        # Example: markdown_content = markdown_content.replace("  username :", "")
        
        # 2.1. Remove Confluence footer text (fallback pattern for markdown)
        # Pattern: "Document generated by Confluence on [date]" followed by "[Atlassian]" link
        confluence_md_pattern1 = r'Document generated by Confluence on [^\n]*\n*\[Atlassian\]\([^)]*\)'
        markdown_content = re.sub(confluence_md_pattern1, '', markdown_content, flags=re.IGNORECASE | re.MULTILINE)
        
        # Alternative pattern: Just the text line
        confluence_md_pattern2 = r'Document generated by Confluence on [^\n]*\n*'
        markdown_content = re.sub(confluence_md_pattern2, '', markdown_content, flags=re.IGNORECASE | re.MULTILINE)
        
        # Remove standalone Atlassian links
        atlassian_md_pattern = r'\[Atlassian\]\([^)]*atlassian[^)]*\)\s*'
        markdown_content = re.sub(atlassian_md_pattern, '', markdown_content, flags=re.IGNORECASE | re.MULTILINE)
        
        # 2.2. Remove breadcrumb navigation links (fallback pattern for markdown)
        # These appear as markdown links at the beginning: [username](index.html) / [Page](file.html)
        
        # More general pattern: remove any navigation-style links at the beginning
        nav_links_pattern = r'^(\[[^\]]+\]\([^)]*\.html\)(\s*/\s*)?)+\s*\n*'
        markdown_content = re.sub(nav_links_pattern, '', markdown_content, flags=re.IGNORECASE | re.MULTILINE)
        
        # 3. Remove reference links sections (numbered lists with document links)
        # Pattern: numbered lists with links to .md files
        reference_links_pattern = r'\n\s*\d+\.\s*\[[^\]]+\]\([^)]*\.md\)[^\n]*(?:\n|$)'
        markdown_content = re.sub(reference_links_pattern, '', markdown_content, flags=re.MULTILINE)
        
        # Remove empty reference sections (lines that are just numbers followed by periods)
        # This cleans up any remaining numbered list artifacts
        empty_ref_pattern = r'\n\s*\d+\.\s*\n'
        markdown_content = re.sub(empty_ref_pattern, '\n', markdown_content, flags=re.MULTILINE)
        
        # Clean up multiple consecutive newlines (more than 2)
        markdown_content = re.sub(r'\n{3,}', '\n\n', markdown_content)
        
        # Additional Markdown postprocessing can be added here
        # Example: markdown_content = markdown_content.replace('unwanted_markdown', '')
        
        return markdown_content.strip()
        
    except Exception as e:
        print(f"âš ï¸ Warning: Error in Markdown postprocessing: {str(e)}")
        return markdown_content

# ============================================================================
# END CUSTOMIZATION SECTION
# ============================================================================

def create_timestamped_output_dir(base_dir=None, prefix="conversion"):
    """
    Create a timestamped output directory.
    
    Args:
        base_dir (str, optional): Base directory path. If None, uses script directory.
        prefix (str): Prefix for the folder name
    
    Returns:
        Path: Path to the created timestamped directory
    """
    if base_dir is None:
        base_dir = Path(__file__).parent
    else:
        base_dir = Path(base_dir)
    
    # Create timestamp string: YYYY-MM-DD_HH-MM-SS
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    
    # Create folder name with timestamp
    folder_name = f"{prefix}_{timestamp}"
    output_dir = base_dir / folder_name
    
    # Create the directory
    output_dir.mkdir(parents=True, exist_ok=True)
    
    return output_dir

def extract_local_html_links(html_content, base_path):
    """
    Extract all local HTML links from HTML content.
    
    Args:
        html_content (str): HTML content to parse
        base_path (Path): Base path for resolving relative links
    
    Returns:
        set: Set of absolute paths to local HTML files
    """
    try:
        # Simple regex to find href attributes
        href_pattern = r'href\s*=\s*["\']([^"\']+)["\']'
        links = re.findall(href_pattern, html_content, re.IGNORECASE)
        
        local_html_files = set()
        
        for link in links:
            # Skip external links, anchors, and non-HTML files
            if link.startswith(('http://', 'https://', 'mailto:', 'tel:', '#')):
                continue
            
            # Remove fragment identifier (#section)
            link = link.split('#')[0]
            
            # Skip empty links
            if not link:
                continue
            
            # Resolve relative path
            if link.startswith('/'):
                # Absolute path from root - skip for local files
                continue
            
            link_path = (base_path / link).resolve()
            
            # Check if it's an HTML file and exists
            if (link_path.suffix.lower() in ['.html', '.htm'] and 
                link_path.exists() and 
                link_path.is_file()):
                local_html_files.add(link_path)
        
        return local_html_files
        
    except Exception as e:
        print(f"âš ï¸ Warning: Error extracting links: {str(e)}")
        return set()

def extract_ordered_html_links(html_content, base_path):
    """
    Extract local HTML links in the order they appear in the HTML content.
    
    Args:
        html_content (str): HTML content to parse
        base_path (Path): Base path for resolving relative links
    
    Returns:
        list: List of absolute paths to local HTML files in order
    """
    try:
        # Regex to find href attributes with their text content
        href_pattern = r'<a[^>]*href\s*=\s*["\']([^"\']+)["\'][^>]*>([^<]*)</a>'
        matches = re.findall(href_pattern, html_content, re.IGNORECASE | re.DOTALL)
        
        ordered_html_files = []
        seen_files = set()
        
        for link, link_text in matches:
            # Skip external links, anchors, and non-HTML files
            if link.startswith(('http://', 'https://', 'mailto:', 'tel:', '#')):
                continue
            
            # Remove fragment identifier (#section)
            clean_link = link.split('#')[0]
            
            # Skip empty links
            if not clean_link:
                continue
            
            # Resolve relative path
            if clean_link.startswith('/'):
                # Absolute path from root - skip for local files
                continue
            
            try:
                link_path = (base_path / clean_link).resolve()
                
                # Check if it's an HTML file and exists
                if (link_path.suffix.lower() in ['.html', '.htm'] and 
                    link_path.exists() and 
                    link_path.is_file() and
                    link_path not in seen_files):
                    ordered_html_files.append((link_path, link_text.strip()))
                    seen_files.add(link_path)
            except:
                continue
        
        return ordered_html_files
        
    except Exception as e:
        print(f"âš ï¸ Warning: Error extracting ordered links: {str(e)}")
        return []

def convert_html_to_markdown_content(html_path):
    """
    Convert HTML file to Markdown content (without writing to file).
    
    Args:
        html_path (Path): Path to HTML file
    
    Returns:
        tuple: (markdown_content, title, success)
    """
    try:
        # Read HTML content
        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
            html_content = f.read()
        
        # Apply custom HTML preprocessing (remove Confluence footers, etc.)
        html_content = apply_custom_html_preprocessing(html_content)
        
        # Extract title from HTML
        title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
        title = title_match.group(1).strip() if title_match else html_path.stem
        
        # Configure html2text converter
        converter = html2text.HTML2Text()
        converter.ignore_links = False
        converter.body_width = 0
        converter.unicode_snob = True
        converter.mark_code = True
        converter.ignore_images = False
        converter.default_image_alt = "Image"
        
        # Convert to markdown
        markdown_content = converter.handle(html_content)
        
        # Apply custom Markdown postprocessing (remove user references, reference links, etc.)
        markdown_content = apply_custom_markdown_postprocessing(markdown_content)
        
        return markdown_content, title, True
        
    except Exception as e:
        print(f"âŒ Error converting {html_path.name}: {str(e)}")
        return "", html_path.stem, False

def create_single_markdown_file(start_path, output_file=None, max_depth=10):
    """
    Create a single combined Markdown file from all linked HTML pages in logical order.
    
    Args:
        start_path (str): Path to starting HTML file (usually index.html)
        output_file (str, optional): Path for output Markdown file
        max_depth (int): Maximum recursion depth to prevent infinite loops
    
    Returns:
        dict: Conversion results summary
    """
    try:
        start_path = Path(start_path).resolve()
        
        if not start_path.exists():
            print(f"âŒ Error: Starting file '{start_path}' not found.")
            return {"success": 0, "failed": 0, "files": []}
        
        if not start_path.suffix.lower() in ['.html', '.htm']:
            print(f"âŒ Error: Starting file must be an HTML file.")
            return {"success": 0, "failed": 0, "files": []}
        
        # Set output file - default to timestamped folder with "combined.md"
        if output_file is None:
            output_dir = create_timestamped_output_dir(prefix="single_conversion")
            output_file = output_dir / "combined.md"
        else:
            output_file = Path(output_file)
            # If only filename provided, use timestamped directory
            if output_file.parent == Path('.'):
                output_dir = create_timestamped_output_dir(prefix="single_conversion")
                output_file = output_dir / output_file.name
        
        # Create output directory
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        print(f"ðŸš€ Creating single combined Markdown file from: {start_path.name}")
        print(f"ðŸ“ Output file: {output_file}")
        print(f"ðŸ” Maximum depth: {max_depth}\n")
        
        # Track processed files and content
        processed_files = set()
        combined_content = []
        results = {"success": 0, "failed": 0, "files": []}
        
        def process_file_depth_first(file_path, depth=0, parent_title=""):
            """Recursively process files in depth-first order."""
            if file_path in processed_files or depth > max_depth:
                return
            
            processed_files.add(file_path)
            
            # Convert current file
            print(f"{'  ' * depth}ðŸ”„ Processing: {file_path.name}")
            markdown_content, title, success = convert_html_to_markdown_content(file_path)
            
            if success:
                # Add section header
                indent = "#" * (depth + 1)
                section_header = f"\n{indent} {title}\n\n"
                if depth == 0:
                    section_header = f"# {title}\n\n"
                
                # Add separator for better readability
                if combined_content:  # Not the first file
                    combined_content.append("\n---\n")
                
                combined_content.append(section_header)
                combined_content.append(markdown_content)
                
                results["success"] += 1
                print(f"{'  ' * depth}âœ… Added: {title}")
                
                # Find and process linked files in order
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        html_content = f.read()
                    
                    ordered_links = extract_ordered_html_links(html_content, file_path.parent)
                    
                    for linked_file, link_text in ordered_links:
                        if linked_file not in processed_files:
                            process_file_depth_first(linked_file, depth + 1, link_text)
                            
                except Exception as e:
                    print(f"âš ï¸ Warning: Could not process links from {file_path.name}: {str(e)}")
            else:
                results["failed"] += 1
        
        # Start processing from the index file
        process_file_depth_first(start_path)
        
        # Write combined content to file
        if combined_content:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(''.join(combined_content))
            
            results["files"].append(str(output_file))
            
            # Create a summary
            summary_data = {
                "start_file": str(start_path),
                "output_file": str(output_file),
                "total_files_processed": len(processed_files),
                "successful_conversions": results["success"],
                "failed_conversions": results["failed"],
                "processed_files": [str(f) for f in processed_files]
            }
            
            summary_file = output_file.parent / "single_conversion_summary.json"
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary_data, f, indent=2)
        
        # Print summary
        print(f"\nðŸ“Š Single File Conversion Summary:")
        print(f"   ðŸ“„ Total files processed: {len(processed_files)}")
        print(f"   âœ… Successfully converted: {results['success']}")
        print(f"   âŒ Failed conversions: {results['failed']}")
        print(f"   ðŸ“„ Combined output file: {output_file}")
        
        return results
        
    except Exception as e:
        print(f"âŒ Error during single file conversion: {str(e)}")
        return {"success": 0, "failed": 0, "files": []}

def generate_hierarchical_filename(title, hierarchy_numbers, max_title_length=120):
    """
    Generate a hierarchical filename with numbering.
    
    Args:
        title (str): Title of the page
        hierarchy_numbers (list): List of numbers representing hierarchy (e.g., [1, 2] for 1.2)
        max_title_length (int): Maximum length for title part
    
    Returns:
        str: Formatted filename like "1.2_Page-Title.md"
    """
    # Clean the title for filename use
    clean_title = re.sub(r'[<>:"/\\|?*]', '', title)  # Remove invalid filename chars
    clean_title = re.sub(r'\s+', '-', clean_title.strip())  # Replace spaces with hyphens
    
    # CUSTOMIZATION: Remove user-specific prefixes and associated symbols
    # You can add custom patterns here to remove specific username prefixes
    # Example: clean_title = re.sub(r'^username\s*[-:]\s*', '', clean_title, flags=re.IGNORECASE)
    # Remove any leading dashes that might be left over
    clean_title = clean_title.lstrip('-').strip()
    
    # Smart truncation - cut at word boundaries if too long
    if len(clean_title) > max_title_length:
        # Try to cut at the last hyphen within the limit
        truncated = clean_title[:max_title_length]
        last_hyphen = truncated.rfind('-')
        
        if last_hyphen > max_title_length * 0.7:  # If we can keep at least 70% and cut at word boundary
            clean_title = truncated[:last_hyphen]
        else:
            # If no good word boundary, just truncate and add indicator
            clean_title = truncated.rstrip('-') + '...'
    
    # Generate hierarchy prefix with zero-padding
    if not hierarchy_numbers:
        hierarchy_prefix = "00"
    else:
        # Format each number with zero-padding (e.g., 01, 02, 03...)
        padded_numbers = [f"{num:02d}" for num in hierarchy_numbers]
        hierarchy_prefix = ".".join(padded_numbers)
    
    # Combine hierarchy and title
    filename = f"{hierarchy_prefix}_{clean_title}.md"
    
    return filename

def convert_html_to_markdown_with_link_updates(html_path, output_path, converted_files):
    """
    Convert HTML to Markdown and update internal links to point to .md files.
    
    Args:
        html_path (Path): Path to HTML file
        output_path (Path): Path for output Markdown file
        converted_files (dict): Mapping of HTML files to their MD counterparts
    
    Returns:
        bool: True if conversion successful
    """
    try:
        print(f"ðŸ”„ Converting: {html_path.name}")
        
        # Read HTML content
        with open(html_path, 'r', encoding='utf-8', errors='ignore') as f:
            html_content = f.read()
        
        # Apply custom HTML preprocessing (remove Confluence footers, etc.)
        html_content = apply_custom_html_preprocessing(html_content)
        
        # Configure html2text converter
        converter = html2text.HTML2Text()
        converter.ignore_links = False
        converter.body_width = 0
        converter.unicode_snob = True
        converter.mark_code = True
        converter.ignore_images = False
        converter.default_image_alt = "Image"
        
        # Convert to markdown
        markdown_content = converter.handle(html_content)
        
        # Update internal links to point to .md files
        def update_link(match):
            link_text = match.group(1)
            link_url = match.group(2)
            
            # Skip external links
            if link_url.startswith(('http://', 'https://', 'mailto:', 'tel:')):
                return match.group(0)
            
            # Remove fragment identifier for path resolution
            base_url = link_url.split('#')[0]
            fragment = '#' + link_url.split('#')[1] if '#' in link_url else ''
            
            if base_url:
                # Resolve the path relative to the current HTML file
                try:
                    target_html_path = (html_path.parent / base_url).resolve()
                    
                    # Check if we've converted this file
                    if target_html_path in converted_files:
                        # Update to point to the .md file
                        md_path = converted_files[target_html_path]
                        relative_md_path = os.path.relpath(md_path, output_path.parent)
                        # Normalize path separators for markdown
                        relative_md_path = relative_md_path.replace('\\', '/')
                        return f'[{link_text}]({relative_md_path}{fragment})'
                except:
                    pass
            
            # Keep original link if we can't resolve it
            return match.group(0)
        
        # Update markdown links
        markdown_content = re.sub(r'\[([^\]]*)\]\(([^)]+)\)', update_link, markdown_content)
        
        # Apply custom Markdown postprocessing (remove user references, reference links, etc.)
        markdown_content = apply_custom_markdown_postprocessing(markdown_content)
        
        # Create output directory if needed
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Write markdown file
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"âœ… Converted: {html_path.name} â†’ {output_path.name}")
        return True
        
    except Exception as e:
        print(f"âŒ Error converting {html_path.name}: {str(e)}")
        return False

def recursive_html_to_markdown(start_path, output_dir=None, max_depth=10):
    """
    Recursively convert HTML files starting from a given file.
    
    Args:
        start_path (str): Path to starting HTML file (usually index.html)
        output_dir (str, optional): Output directory for markdown files
        max_depth (int): Maximum recursion depth to prevent infinite loops
    
    Returns:
        dict: Conversion results summary
    """
    try:
        start_path = Path(start_path).resolve()
        
        if not start_path.exists():
            print(f"âŒ Error: Starting file '{start_path}' not found.")
            return {"success": 0, "failed": 0, "files": []}
        
        if not start_path.suffix.lower() in ['.html', '.htm']:
            print(f"âŒ Error: Starting file must be an HTML file.")
            return {"success": 0, "failed": 0, "files": []}
        
        # Set output directory - default to timestamped folder in script directory
        if output_dir is None:
            output_dir = create_timestamped_output_dir(prefix="multi_conversion")
        else:
            output_dir = Path(output_dir)
            # If only a folder name provided, create timestamped version
            if not output_dir.is_absolute() and '/' not in str(output_dir) and '\\' not in str(output_dir):
                base_dir = Path(__file__).parent
                timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
                output_dir = base_dir / f"{output_dir}_{timestamp}"
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ðŸš€ Starting recursive conversion from: {start_path.name}")
        print(f"ðŸ“ Output directory: {output_dir}")
        print(f"ðŸ” Maximum depth: {max_depth}\n")
        
        # Track files with hierarchy information
        processed_files = set()
        converted_files = {}  # HTML path -> MD path mapping
        hierarchy_map = {}  # HTML path -> hierarchy numbers
        results = {"success": 0, "failed": 0, "files": []}
        
        def process_files_hierarchically(file_path, hierarchy_numbers, depth=0):
            """Recursively process files and assign hierarchy numbers."""
            if file_path in processed_files or depth > max_depth:
                return
            
            processed_files.add(file_path)
            hierarchy_map[file_path] = hierarchy_numbers.copy()
            
            # Extract title for filename
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    html_content = f.read()
                
                # Get title from HTML
                title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
                if title_match:
                    title = title_match.group(1).strip()
                else:
                    title = file_path.stem
                
                # Generate hierarchical filename
                filename = generate_hierarchical_filename(title, hierarchy_numbers)
                output_path = output_dir / filename
                converted_files[file_path] = output_path
                
                # Find and process linked files in order
                if depth < max_depth:
                    ordered_links = extract_ordered_html_links(html_content, file_path.parent)
                    
                    for index, (linked_file, link_text) in enumerate(ordered_links, 1):
                        if linked_file not in processed_files:
                            # Create new hierarchy numbers for child
                            child_hierarchy = hierarchy_numbers + [index]
                            process_files_hierarchically(linked_file, child_hierarchy, depth + 1)
                
            except Exception as e:
                print(f"âš ï¸ Warning: Could not process {file_path.name}: {str(e)}")
                # Fallback filename
                filename = generate_hierarchical_filename(file_path.stem, hierarchy_numbers)
                output_path = output_dir / filename
                converted_files[file_path] = output_path
        
        # Start processing from index file (hierarchy [])
        print(f"ðŸ” Discovering files and planning hierarchy...\n")
        process_files_hierarchically(start_path, [])
        
        print(f"ðŸ“‹ Found {len(converted_files)} HTML files to convert with hierarchy:\n")
        
        # Sort by hierarchy for display
        sorted_files = sorted(converted_files.items(), 
                            key=lambda x: hierarchy_map.get(x[0], []))
        
        for html_path, md_path in sorted_files:
            hierarchy_nums = hierarchy_map.get(html_path, [])
            if hierarchy_nums:
                # Format with zero-padding for consistency with filenames
                padded_nums = [f"{num:02d}" for num in hierarchy_nums]
                hierarchy_str = ".".join(padded_nums)
            else:
                hierarchy_str = "00"
            print(f"   {hierarchy_str}: {html_path.name} â†’ {md_path.name}")
        
        print()
        
        # Convert all files with updated links
        for html_path, md_path in converted_files.items():
            if convert_html_to_markdown_with_link_updates(html_path, md_path, converted_files):
                results["success"] += 1
                results["files"].append(str(md_path))
            else:
                results["failed"] += 1
        
        # Create a summary file
        summary_file = output_dir / "conversion_summary.json"
        summary_data = {
            "start_file": str(start_path),
            "output_directory": str(output_dir),
            "total_files": len(converted_files),
            "successful_conversions": results["success"],
            "failed_conversions": results["failed"],
            "converted_files": {str(k): str(v) for k, v in converted_files.items()},
            "hierarchy_mapping": {
                str(k): {
                    "hierarchy_numbers": hierarchy_map.get(k, []),
                    "hierarchy_string": (
                        ".".join(f"{num:02d}" for num in hierarchy_map.get(k, [])) 
                        if hierarchy_map.get(k, []) else "00"
                    ),
                    "output_file": str(v)
                }
                for k, v in converted_files.items()
            }
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2)
        
        # Print summary
        print(f"\nðŸ“Š Conversion Summary:")
        print(f"   ðŸ“„ Total files found: {len(converted_files)}")
        print(f"   âœ… Successfully converted: {results['success']}")
        print(f"   âŒ Failed conversions: {results['failed']}")
        print(f"   ðŸ“ Output directory: {output_dir}")
        print(f"   ðŸ“‹ Summary saved to: {summary_file}")
        print(f"   ðŸ”¢ Files organized with hierarchical numbering")
        print(f"      Example: 00_index.md â†’ 01_guide.md â†’ 01.01_installation.md")
        
        return results
        
    except Exception as e:
        print(f"âŒ Error during recursive conversion: {str(e)}")
        return {"success": 0, "failed": 0, "files": []}

def interactive_mode():
    """Interactive mode for recursive conversion."""
    print("=== Recursive HTML to Markdown Converter ===\n")
    
    while True:
        start_file = input("Enter path to starting HTML file (usually index.html) or 'quit': ").strip()
        
        if start_file.lower() in ['quit', 'q', 'exit']:
            print("Goodbye!")
            break
        
        start_file = start_file.strip('"\'')
        if os.name == 'nt':  # Windows
            start_file = start_file.replace('/', '\\')
        
        if not os.path.exists(start_file):
            print(f"âŒ Error: File '{start_file}' not found.\n")
            continue
        
        print(f"\nStarting file: {start_file}")
        
        # Output will be saved to timestamped directories automatically
        script_dir = Path(__file__).parent
        print(f"ðŸ“ Output will be saved to timestamped folders in: {script_dir}")
        
        # Ask for output format
        print("\nOutput format options:")
        print("1. Single combined Markdown file (recommended)")
        print("2. Multiple Markdown files (one per HTML page)")
        
        while True:
            format_choice = input("Choose output format (1 or 2): ").strip()
            if format_choice in ['1', '2']:
                break
            print("Please enter 1 or 2.")
        
        single_file_mode = (format_choice == '1')
        
        # Ask for max depth
        max_depth_input = input("Maximum recursion depth (press Enter for 10): ").strip()
        try:
            max_depth = int(max_depth_input) if max_depth_input else 10
            max_depth = max(1, min(max_depth, 50))  # Limit between 1 and 50
        except ValueError:
            max_depth = 10
        
        print(f"\nðŸš€ Starting recursive conversion...")
        
        if single_file_mode:
            # Create single combined file with timestamped directory
            results = create_single_markdown_file(start_file, None, max_depth)
        else:
            # Create multiple files with timestamped directory
            results = recursive_html_to_markdown(start_file, None, max_depth)
        
        if results["success"] > 0:
            print(f"\nðŸŽ‰ Conversion completed! Check the output directory for your markdown files.")
        else:
            print(f"\nâŒ Conversion failed. Please check the error messages above.")
        
        print("\nGoodbye!")
        break

def main():
    """Main function to handle command line arguments and execute conversions."""
    # Check if any command line arguments were provided
    if len(sys.argv) == 1:
        # No arguments provided, start interactive mode
        interactive_mode()
        return
    
    parser = argparse.ArgumentParser(
        description="Recursively convert HTML files to Markdown starting from index.html",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive mode (no arguments)
  python recursive_html_converter.py
  
  # Convert to multiple files (default)
  python recursive_html_converter.py index.html
  
  # Convert to single combined Markdown file
  python recursive_html_converter.py index.html -s
  
  # Convert to single file with custom output name
  python recursive_html_converter.py index.html -s -o complete_guide.md
  
  # Convert with custom output directory (multiple files)
  python recursive_html_converter.py index.html -o markdown_docs
  
  # Limit recursion depth
  python recursive_html_converter.py index.html -d 5
        """
    )
    
    parser.add_argument('start_file', help='Starting HTML file (usually index.html)')
    parser.add_argument('-o', '--output', help='Output directory/file path (default: timestamped directory in script location)')
    parser.add_argument('-d', '--depth', type=int, default=10, 
                       help='Maximum recursion depth (default: 10)')
    parser.add_argument('-s', '--single', action='store_true',
                       help='Create single combined Markdown file instead of multiple files')
    
    args = parser.parse_args()
    
    if args.single:
        # Single file mode
        if args.output:
            output_file = args.output
        else:
            # Use automatic timestamped directory
            output_file = None
        
        results = create_single_markdown_file(args.start_file, output_file, args.depth)
    else:
        # Multiple files mode
        # If no output specified, use automatic timestamped directory
        output_dir = args.output if args.output else None
        results = recursive_html_to_markdown(args.start_file, output_dir, args.depth)
    
    sys.exit(0 if results["failed"] == 0 else 1)

if __name__ == "__main__":
    main()